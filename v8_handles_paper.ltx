%&v8_handles_paper_preamble
\endofdump

\begin{document}

\begin{abstract}
Some programming language Virtual Machines (VMs) use \emph{handles}, that is
wrappers around pointers to heap objects, to allow the root set of GCed
(Garbage Collected) objects to be precisely enumerated. Other VMs use
\emph{direct references} to heap objects, requiring the C/C++ call stack to
be conservatively scanned for values that appear to be pointers to heap
objects. In this paper we study the performance difference
between these two approaches by migrating around half of V8 from handles to
pointers: we show that, even though our partial migration imposes
additional costs and our conservative stack scanner is naive, direct
references are at least as fast as handles.
\end{abstract}


\maketitle


\section{Introduction}

A major challenge for Garbage Collection (GC) is finding all the \emph{roots},
that is the starting set of references into the object graph. Ideally, GCs
would be able to precisely enumerate the root set. However, current languages
and compilers make this difficult, or impossible, as pointers
to objects can be `lost' on the call stack: to compensate, the call stack
must be \emph{conservatively scanned}, looking for pointers to objects on
the heap. This over-approximates the true root set as some non-pointers
may have a value which also happens to be a pointer to an object.
To avoid conservative scanning, one can use \emph{handles}, that
is wrappers around object references, such that the handles allow \emph{precise}
enumeration of the root set.

\label{comprehensive_handles}
In this paper, we consider this long-standing implementation and design trade-off
in the context of programming language Virtual Machines (VMs). Some VMs
(e.g.~HotSpot, SpiderMonkey, V8) use handles; some (e.g.~JavaScriptCore and
those VMs using the Boehm-Demers-Weiser GC) use \emph{direct references}
(i.e.~`normal pointers'), and partial
or complete conservative scanning. There is little debate about the relative
ergonomics: handles are more difficult to use, and some kind of misuse
lead to security flaws. We also now have good evidence
that conservative scanning of direct references tends to only slightly
over-approximate the root set~\cite{shahriyar14fast}. However, there is no
clear evidence for the run-time costs of handles relative to direct references
and conservative scanning. Without such an understanding, it is difficult to
state with confidence whether direct references or handles are the better trade-off.

In this paper we examine the widely used JavaScript VM V8, which has
been designed from its inception to use handles. Our hypothesis is
that moving from handles to direct references (and thus also conservative scanning) will improve V8's performance.
However, moving wholesale from handles to direct references is impractical on a
codebase of V8's size (~1.75m lines of code) \jake{does this include torque files?}.
Our first challenge was thus
to find a practical means for gradually migrating parts of V8 from handles to
direct references (Section~\ref{sec:migration}). Having then migrated a
around half of V8 from handles to direct references,
and implemented a naive conservative stack
scanner, we then run several standard V8 benchmark
suites, and find that direct references are at least as fast as handles
(\cref{sec:evaluation}). This
suggests that a complete migration of V8 to direct references, and a more
sophisticated conservative stack scanner, will improve V8's performance
while reducing a class of security flaw.


\section{Background}
\label{sec:background}

In this section we provide a brief overview of direct references and handles in
the context of VMs. To help make this concrete, we do so for a
hypothetical VM, written in C, that implements an object orientated language.
In this VM, every language-level object is an instance of a \texttt{struct} called
\texttt{Obj}, which we will assume has a single field \texttt{data} of type
\texttt{int}. We assume that the VM contains a GC that runs in a thread
concurrently with other VM threads: thus the GC can free the memory of any
unreachable \texttt{Obj} at any point.


\subsection{Direct References}

The most `natural' way to program our hypothetical VM is to use direct
references. We assume that the GC exposes a single function \texttt{Obj
*gc\_new\_obj()}, which returns a pointer to a block of memory with a freshly
initialised \texttt{Obj}. When we want to create a new object, we call
\texttt{gc\_new\_obj}, assign the result to a local variable, and then operate
upon its \texttt{data} field:

\begin{lstlisting}[language=C]
Obj *n = gc_new_obj();
n.data = 1;
\end{lstlisting}

As this example suggests, `direct references' is our term for
what many would see as the `normal' way of programming in C. Unlike most
normal C programming, however, our VM must consider the concurrent GC:
it can run immediately after line 1 has executed, but
before line 2 has executed. If the GC were not to notice that \texttt{n}
is pointing to a live \texttt{Obj}, it would free that \texttt{Obj}'s memory,
causing the program to misbehave (e.g.~the program might segfault or overwrite
a portion of memory now used in another thread). In other words,
at that point in the program, \texttt{n} is a \emph{root}, that
is an entry point into the graph of objects created by the VM. For
our purposes, we can consider the root set to be all references to
objects stored in C-level variables, bearing
in mind that at run-time such variables may be stored as part of a
function frame on the C call stack\footnote{There are
other hiding places for such references, ranging from thread-locals to
registers to intermediate variables: for introductory purposes, these can be considered minor
variations on the C call stack problem.}.

The ideal, and universal, solution would be for C's language specification to
allow \emph{introspection} of a function's frame layout, telling it,
for example, where each local variable lives within a frame. For each
frame on the stack, the GC could then read the value of only those variables whose
compile-time type is \texttt{Obj*}, considering the objects they reference as roots.
Local variables of other types, which are not
of interest to the GC, would be ignored. This would allow the GC to \emph{precisely}
enumerate the GC roots (i.e.~to include all live objects as root while not
including any non-live objects as roots).
Unfortunately, no mainstream systems language defines an introspection
capability.

Some compilers instead offer the concept of \emph{stackmaps}, where
the compiler defines a way for a program to introspect the call stack at
specific locations. Stackmaps offer the same functionality as language-defined
introspection, but do so in a way that is not portable between compilers.
Unfortunately, stackmaps are not yet widely available. GCC does not
have any support for stackmaps. LLVM does have some stackmap support,
but (as some of this paper's authors have discovered in other work), it
is experimental, incomplete, and unsupported, with particular problems at
higher optimisation levels. \jake{I don't think the gc root stuff is worth
mentioning, but should we cite the statepoint docs?}
In practise, therefore, VMs using direct references have to turn to
\emph{conservative scanning}, where the C call stack is exhaustively examined for
pointers to instances of \texttt{Obj}. In order for this to work, the GC must
know at what address a thread's stack began, and what address the stack is
currently at. Each aligned word on the stack is then checked to see whether
it references an instance of \texttt{Obj}: if it does, that \texttt{Obj} is
considered a root. Depending on the GC, checking whether an arbitrary word is a
pointer to an \texttt{Obj} can be fairly expensive \laurie{cite?}. Conservative
scanning also inherently
over-approximates the root set, because random words on the stack may
accidentally point to an \texttt{Obj}, keeping it alive longer than necessary.
Fortunately, in practise relatively few objects are kept alive unnecessarily long:
the most extensive study we know of suggests the false detection rate
in Java programs is under 0.01\% of live objects~\cite{shahriyar14fast}.

Objects identified as live via conservative scanning must also be pinned in
memory. Fully conservative GCs, that is GCs who conservatively scan all parts
of memory, from the call stack to the heap, (e.g.~the Boehm-Demers-Weiser
GC~\cite{boehm88garbage}) are fundamentally unable to move any objects in
memory. Most conservative GCs are semi-conservative, that is most references to
objects are determined precisely, with only some roots discovered
conservatively. Semi-conservative GCs must temporarily pin those objects
identified as live by conservative scanning, because they cannot know whether
such references can be safely rewritten or not.

Conservative scanning also occupies an odd position in software. Technically speaking, the
way it works violates the rules of most languages, most compilers, and most
operating systems. Furthermore, some programming techniques
(e.g.~XOR lists) thoroughly defeat it.
However, because conservative scanning is widely used (e.g.~the well-known
Boehm-Demers-Weiser GC~\cite{boehm88garbage} and WebKit~\cite{pizlo17riptide}),
it is well supported in practise.


\subsection{Handles}
\label{handles_general}

An alternative approach to direct references and conservative scanning
has become known as handles. These were first first introduced in
recognisable form by~\cite{brooks84trading}, though the term `handles' was only
used later. Modern VMs use handles in two distinct ways: in this section
we will consider \emph{comprehensive handles}, as found in e.g.~\cite{kalibera11handles}
and SpiderMonkey~\cite{spidermonkey18rooting} \laurie{checking with shu-yu}
(in Section~\ref{handles_in_v8} we will consider the other kind of handles).

Comprehensive handles add a level of indirection to all object references, with
the indirection being the `handle'. Handles are stored at a fixed point in
memory, with one handle per object. In the context of our hypothetical
VM, this means that we never store references to \texttt{Obj} directly, instead
storing references to a \texttt{Handle} struct. When the VM wants to access an
\texttt{Obj} it must \emph{tag} the corresponding \texttt{Handle}; this informs
the GC that the \texttt{Handle} is a root. When the VM is finished with the
\texttt{Obj} it must untag the corresponding \texttt{Handle}. We maintain a tag
count for each \texttt{Handle}, as it may be tagged multiple times before being
untagged: when the tag count goes to zero, the \texttt{Handle} is no longer a
root.

Our hypothetical GC needs three altered/new functions: \texttt{Handle
*gc\_new\_obj()} returns a pointer to a handle, where the handle points to a
freshly initialised \texttt{Obj}; \texttt{Obj *tag(Handle *)} which increments
a handle's tag count by one and returns a pointer to the underlying
\texttt{Obj}; and \texttt{void untag(Handle *)} which decrement the handle's
tag count by one. For example:

\begin{lstlisting}[language=C]
Handle *h = gc_new_obj();
Obj *o = tag(h);
o.data = 1;
untag(h);
\end{lstlisting}

Handles have important advantages. First, tagging and untagging allows
the GC to precisely determine the root set by iterating over all handles
and considering as a root any handle with a tag count greater than 0.
Second, handles are fully portable, do not trifle with undefined behaviour,
and require no explicit language or compiler support.
Third, any \texttt{Handle} with a zero tag count can have its
underlying \texttt{Obj} safely moved. In other words, handles
make it trivial to write precise, (almost fully) moving GCs. Comprehensive handles also
have the virtue that moving an \texttt{Obj} requires only updating its
corresponding \texttt{Handle}.

There are however disadvantages. First, any handle API is easy to misuse:
forgetting to \texttt{tag} a handle or \texttt{untag}ing a handle twice leads
to undefined behaviour. Finding such API misuse is notoriously hard, and VMs
such as V8 have developed home-grown linters that scan for obvious API misuse,
though none that we know of can catch all possible kinds of API misuse. Second,
handles' double level of indirection implies at least some performance loss
relative to the single indirection of direct references. Third, handles have
additional run-time costs beyond those of double indirection: each handle
consumes memory; since handles cannot move, they can cause memory
fragmentation; and tagging / untagging a handle requires memory reads and writes.


\section{Handles in V8}

V8 is a popular JavaScript and WebAssembly VM, embedded in all Chromium-based browsers like Google's Chrome and various other prominent software systems like node.js, deno, Electron, etc.
In this section we introduce some general V8 background, before describing V8's use of handles.

V8 is largely written in C++.
A single V8 process can run many unrelated programs by encapsulating them in \emph{isolates} which all come with their own private heaps.
V8's heap is separated into the young and the old generation.
It can perform a young generation collection to free up objects in the young generation or a full collection to free up objects in both the young and old generation.
The young generation uses a semi-space strategy collected by a parallel variant of Cheney's algorithm~\cite{cheney70nonrecursive}.
Objects which have survived one young generation collection are promoted to the old generation.
A full collection collects both the old and the young generation using a mark-sweep-compact strategy.
Marking and sweeping are performed concurrently~\cite{degenbaev19}.
Compaction is performed in parallel during the main pause.
V8 allows its embedder to schedule garbage collections during known program idle times~\cite{degenbaev16idle} which is used by Chrome to improve the user experience.

\label{handles_in_v8}

V8 uses handles to determine the root set of objects. Unlike the simple
VM described in \cref{comprehensive_handles}, V8 uses \emph{partial handles}:
that is, it uses direct references in those places where it can precisely track
them (e.g.~object slots), and handles in those places where it would otherwise
lose track of roots (e.g.~stack references). Although V8 has various kinds
of handles (e.g.~root handles, persistent handles, eternals, and traced
reference handles), most of these have very narrow, specific uses: we focus in
this paper exclusively on `normal' handles.

Comprehensive handles as described in \cref{comprehensive_handles} persist
throughout a VM's lifetime, with each heap object having a single handle
pointing to it. In contrast, V8's partial handles are temporary, being
regularly created and destroyed: any object may, both at any given point
and over its lifetime, have
multiple handles pointing to it. This has a significant impact on the way
handles are used within V8.

In essence, when V8's C++ code wishes to operate on a JavaScript object, it
must create a handle to it, destroying that handle when it is finished. This
can be thought of as a kind of `lock': the handle guarantees that the object
is kept alive while C++ code works upon object, without having to worry that
pointers to the object will be `lost' on the C++ call stack. As with other VMs
using this style of partial handles, V8 faces two challenges: how to make
partial handles ergonomic for the programmer; and how to make their regular
creation and destruction fast.

The C++ \lstinline{Handle} class is used pervasively throughout V8: most
functions that operate on heap objects either take or return \lstinline{Handle}
instances. The \lstinline{Handle} class exposes a pointer-like API (e.g.~the
dereferencing `\texttt{*}' operator on a handle returns an \lstinline{Object})
that makes it relatively transparent in use. A \lstinline{Handle} instance
contains a pointer to the underlying handle but no additional storage
(i.e.~\lstinline{size_of(Handle<T>) == size_of(T*)}, giving programmers
confidence about handle storage and moving costs.

A challenge with partial handles is knowing when they are no longer needed. To
help with this, V8 uses \emph{handle scopes}, which can be thought of as pools
of handles: newly created handles are automatically placed in the `current'
handle scope; and when a handle scope is no longer needed all of the handles in
it are destroyed. When V8 code creates a new \lstinline{HandleScope} instance,
that handle scope is pushed onto a stack, whose topmost element is the
current handle scope. C++'s RAII
mechanism is used to automatically destroy \lstinline{HandleScope} instances,
which also destroys all the handles it contains.

Handle scopes ensure that C++ code dealing with objects remains relatively
terse, and reduces several opportunities for programmer mistakes. However, C++
code must carefully ensure that it does not leak a direct pointer to an object
beyond the lifetime of the handle scope that references that object --- doing
so causes undefined behaviour. V8 has various `lints' to detect many such
mistakes, one such example is \emph{gcmole}, a tool used to find uses of
\lstinline{Handle} instances in the V8 codebase which may be unsafe for GC.
However, they cannot capture all
possible misuses: some misuses are later detected in debug builds, but some
misuses end up in release code, where they can become a significant security
concern.

\begin{figure}
\begin{lstlisting}[language=C++,
  caption={A simplified example of V8's C++ code, showing the use of handles.
  \texttt{main} creates a handle scope (line 2), and then calls \texttt{foo}
  which creates a further handle scope (line 7). Two new heap objects are
  created, each of which produces a handle (lines 8 and 9), ensuring that both
  objects are considered as roots. \texttt{foo} cannot directly return a handle
  to its caller, as that handle will be automatically destroyed when the handle
  scope is destroyed by RAII at the end of the function call. The
  \texttt{CloseAndEscape} method adds a new handle to \texttt{main}'s handle
  scope pointing to the \texttt{"a"} string, and passes a reference to that
  handle to the caller, allowing the underlying object to safely `escape' the
  handle scope it was created in.},
  label=handles_example]
void main() {
  HandleScope scope(GetIsolate());
  Handle<String> str = foo();
}

Handle<String> foo() {
  HandleScope scope(GetIsolate());
  Handle<String> a = NewString("a");
  Handle<String> b = NewString("b");
  return scope.CloseAndEscape(a);
}
\end{lstlisting}
\end{figure}

\cref{handles_example} shows a simplified snippet of V8's C++ code,
demonstrating how \lstinline{Handle}s and \lstinline{HandleScope}s interact. In this example, two handle scopes
are created, with the second handle scope wanting to return a handle that
outlives its handle scope. Handle scopes expose a \texttt{CloseAndEscape}
method which allow a handle to be safely moved to the parent handle scope.

Handle scopes are an important performance optimisation, since destroying
a handle scope causes its backing storage containing multiple handles
to be destroyed in one go: neither individual deallocation nor any form
of handle compaction is necessary.

A further optimisation is based on the observation that within a given handle
scope it is common for the same object to be referenced multiple times.
Although it is always correct to create multiple handles to the same object, it
is inefficient, since each handle requires memory, requiring the storage area
pointed to by a handle scope to be enlarged. When a handle for an object is
requested, V8 thus performs a linear scan through the current handle scope to see if an existing
handle to that object is present, only creating a new handle if none exists.

Although in most of this paper we consider V8 in isolation, we are particularly
interested in how our changes affect V8 when running browser code. That means
that we also need to consider the interactions between V8 and \emph{Blink},
Chrome's rendering engine. V8 exposes a simplified \lstinline{Handle}-like
class called \lstinline{Local}, which also uses two levels of indirection, to
`external' applications such as Blink. Moving objects between Blink and V8
requires converting to/from \lstinline{Handle}s/\lstinline{Local}s.

% dump
% \begin{itemize}
%   \item There's many different kind of handles in V8. They have two purposes: (a) allow the GC to find those object references and (b) allow the GC to relocate objects.
%   \item In V8, handles are an explicit C++ mechanism/construct to maintain this information for the GC
%   \item There's root handles (should probably have a different name) that that maintain this information per handle (costly, but flexible as each handle can be created/destroyed individually) -- the paper doesn't care about those
%   \item There's handles (the interesting ones) that are bound to handle blocks which are maintained in a chain
%   \item The chain itself is dynamic (the list of blocks depends on branches taken at runtime)
%   \item The chain is mostly maintained via leveraging C++ lexical scopes and using RAII to add and remove blocks. Used this way, the handle scopes create precise stack maps.
%   \item Handles automatically register themselves in the current active scope by allocating one entry in such a block
%   \item A handle always refers to an object via the handle block.
%   \item The top of the dynamic chain is known to the GC. The top is identified by a single V8 isolate.
%   \item This way the GC can (a) find such handles and (b) relocate them
%   \item This is much cheaper than the individual root handle version but comes with the restriction to (a) require setting up handle scopes in some stack-like fashion and (b) not using handles that have had their scopoe closed and block removed
%   \item Using a handle that has been invalidated (scopes has been closed and thus block removed) is undefined behavior and results in (likely) crashes in debug builds
% \end{itemize}


\section{Gradually Migrating from Handles to Direct References}
\label{sec:migration}

Migrating from handles to direct references sounds like it should be easy, but
on a codebase of V8's size and complexity turns out to be rather hard.


\subsection{Migration Strategy}

Simplifying slightly, our first migration attempt tried to move the codebase
wholesale from handles to direct references --- after considerable effort this
failed. This was due to a combination of factors but two are of particular
note. First, many parts of V8 quite reasonably make use of implicit properties
of handles that are not captured by the type system or API: these must be manually inspected and altered to work
correctly with direct references. Second, when we inevitably hit bugs due to
our changes, we found it impractical to work out which of our many changes
caused a particular problem.

This paper is the result of our second migration attempt. We used the
experience from our first failed attempt to devise a scheme that allows us to
gradually migrate small portions of V8 from handles to direct references.
The migration is `hidden' behind a compile-time flag that is false by default,
allowing us to upstream many of our changes without affecting mainstream
users.

The most obvious part of our gradual migration approach is to realise that
handles and direct references must be able to co-exist for as long as the
migration takes. We thus introduced a new C++ class which we will refer to in
this paper as \lstinline{DirectRef}\footnote{The actual name of this class is
the confusing, and contradictory, \lstinline{DirectHandle}. Our past selves
have much to apologise for, but they are no longer around to do so.} which
stores a direct reference to objects on the heap while exposing the same
API as \lstinline{Handle}.

In many cases, migrating from handles to direct references simply requires
replacing \lstinline{Handle} with \lstinline{DirectRef}. However, many parts of
V8 make assumptions about handles, normally for performance reasons, that are
not encoded in the \lstinline{Handle} API.

For example, array buffers and string builders are used extensively by
JavaScript code. Implemented naively these would require a fresh handle to be
created each time they are resized \laurie{and presumably all handle scopes in the stack of handle scopes have to be searched?}. To improve performance\footnote{When we started our first migration attempt, we had to
look for challenging parts of the code like this manually. Coincidentally,
V8 has since developed a \lstinline{Handle<T>::PatchValue} API which captures
all the challenging places we know of in a way that makes finding them fairly easy.}, a handle's
intermediate pointer is modified in-place so that this propagates to other handles
using this value.

Similarly, the parts of V8 that enable a transition from C++ code to
JIT-compiled machine code and back are written in a macro-assembler
which is translated into each platform's assembly format. The
macro-assembler does not refer to the \lstinline{Handle}
class directly but generates code which implicitly follows handle conventions.
The macro assembly codes need to be carefully checked and edited by hand and
cannot be done partially: i.e. anything calling into them must be either direct
or handle, it can't be a mix and match.

Another difficulty was balancing the long-term ergonomics of direct references
with the temporary needs of handles. The chief problem is that the
\lstinline{Handle} class's constructors need a reference to an isolate (to
allocate the actual handle in), but the \lstinline{DirectRef} class's
constructors do not. When moving from a migrated portion of code using
\lstinline{DirectRef} to unmigrated code using \lstinline{Handle}, we thus do
not know which isolate to create the new handle in. We could have required
\lstinline{DirectRef} to store a reference to an isolate, but this would be
awkward and muddy performance comparisons. Instead, we assume that a
\lstinline{Handle} created from a \lstinline{DirectRef} lives in the current
isolate. When this assumption is incorrect (for example, when the JIT compiler
hands compilation off to another thread), we almost always get an immediate
crash. Fixing such a crash can be tedious, as the right isolate needs to
be threaded through (potentially many) layers.


\subsection{Performance Challenges}
\label{null_check}

Our migration imposes two performance costs while migration is ongoing.

First, since migrated and unmigrated code frequently interact, moving from code
which uses direct references to code using handles requires creating handles.
Occasionally this can create significant bottlenecks where handles are
continually created simply to interface with unmigrated code. This can lead to
the surprising outcome that migrating a small portion of code can create a
significant slowdown while migrating a tiny additional portion of code can
increase performance. Since migration sometimes imposes handle creation costs
on the callers of a function, we were often unsure which callers of a migrated
function were causing performance problems. We eventually developed a simple
`profiler' which pinpoints how often a line of source code creates handles,
allowing us to quickly identify such locations.

Second, V8 uses \emph{tagged pointers} to avoid storing small integers as full
objects on the heap. The tagging is done on the `pointer' from a handle to a
heap object: if the top bit is set to 1, this is really a pointer; if the top
bit is set to 0, it is a small integer. We use the same pointer tagging scheme in
\lstinline{DirectRef}. However, when converting from handles to direct
references, we have to handle \emph{null handles} with care. Null handles are
used in V8 e.g.~as place holders. To avoid allocating backing memory, the
`nullness' is represented by a null pointer to the handle itself. In other
words, a null \lstinline{Handle} instance itself contains a null pointer, but
if a small integer is stored, the \lstinline{Handle} points to a handle whose
tagged pointer has a top bit set to 0. This representation makes sense for
\lstinline{Handle}, but we have to convert null handles to
\lstinline{DirectRef} tagged pointers (whose value is simply one). Converting
from \lstinline{Handle}s to \lstinline{DirectRef}s thus requires an extra
compare-and-branch to deal with null handles correctly: though the cost of this
check is small, it will definitely have a cost in the general case.


\subsection{What We Migrated}
\label{migrated}

When deciding what parts of V8 to migrate, we wanted to choose those parts
whose migration would give us greatest overall confidence in terms of
correctness and performance. That meant that we chose some of the hardest parts
to migrate, and those that are considered as the most likely performance
bottlenecks. We thus migrated most of the following parts of V8:

\begin{enumerate}
  \item The V8 embedder API layer (where V8 interacts with other applications, such as the Blink rendering
engine in Chrome).
  \item The interface between C++ code and V8's JIT compiled
JavaScript runtime.
  \item JavaScript object definitions and implementations.
  \item Intrinsics, builtins, and runtime functions.
\end{enumerate}


\subsection{Conservative Stack Scanning}

Direct references require conservative stack scanning, so we needed to add
a conservative stack scanner to V8. Rather than writing one fully from scratch,
we were able to borrow some parts of the conservative stack scanner built into
Blink's GC \emph{Oilpan}, specifically the platform specific assembly stubs for
spilling registers during a stop-the-world pause.

Our conservative stack scanner tests each word on the call stack to see
whether it's value looks like a pointer to a heap object. We first check if the
value points into a page in the heap; if it doesn't, we can quickly move on.
If the value does point into a page in the heap, we then check if it points to
an object or empty space. If the value does point to an object, we then query
the isolate's heap layout to find out the object's base pointer. Our scanner
is not yet production ready: compared to Oilpan's scanner, it is clear that
it could be much more heavily optimised.


\section{Evaluation}
\label{sec:evaluation}

In order to understand the performance effects of migrating from handles to direct
references, we conducted an experiment on several variants of V8 running the
\speedometer benchmark suite. We first outline our methodology (\cref{methodology})
before examining the results (\cref{results}).


\subsection{Methodology}

\makeatletter
\newcommand*\ExpandableInputTwo[1]{\@@input#1 }
\makeatother

 \begin{table*}[t]
     \begin{tabular}{lll}
 \toprule
         Benchmark & Handles (ms) & Direct Refs (ms) \\
 \midrule
\ExpandableInputTwo{./table_no_stack_compact}
 \bottomrule
 \end{tabular}
\caption{Results from our first experiment: a straightforward comparison
   between handles and direct references with the GC (largely) running as
   normal on \speedometer. This implicitly includes the full costs of both
   handles (creating handles; destroying handles; an extra level of
   indirection) and direct references (conservative stack scanning). We ran
   \speedometer 30 times and report 99\% confidence intervals. These results
   clearly show that, within the margin of errors, V8 partly migrated to direct
   references is no slower than running solely with handles.}
\label{table:nocompact}
\end{table*}

The overall question we would like our experiment to answer is: are handles
or direct references faster? While we can provide an accurate assessment
of handles performance, we are only able to provide a lower bound of
direct references' performance: around half of V8's (large) codebase remains
unmigrated, and the conservative stack scanner we have implemented is naive,
and thus slower than a production version. We are also forced to use a single
generation heap because the young generation scavenger is a moving semi-space
collector, and work to move this to a non-moving generational GC has not yet
been carried out. Fortunately, and despite these restrictions, our experiment
is able to provide valuable insights.

Our first experiment is a straightforward comparison between handles and direct
references, with the GC running as normal (subject to the restrictions noted
earlier). This implicitly includes the full costs of both handles (creating
handles; destroying handles; an extra level of indirection) and direct
references (conservative stack scanning). We disable heap
compaction when a GC is scheduled while there are frames on the C++ stack as it
does not yet support conservative stack scanning.

Our second experiment aims to tease out the mutator performance of handles and
direct references from the costs of running the collector -- in other words,
what are the costs of creating handles \laurie{when a handle scope is
destroyed, is that memory reclaimed by the GC or normal malloc/free?} and
having two levels of indirection? Ideally we would turn the collector off
entirely, but that causes V8's heap (which has a 4GiB maximum size) to
overflow. Instead, we altered V8 to prevent it from running `intermediate'
collections, instead waiting until the heap is completely full to run a
collection: in other words, we accept higher overall throughput at the expense
of occasional latency. For our benchmark suite, this corresponds to a single
collection per process execution.

We evaluate each browser variant on the well known \speedometer browser benchmark
suite~\cite{speedometer} using Chrome's Crossbench benchmark runner. \speedometer
consists of 16 benchmarks (or `stories'), each of which simulates common
user interactions in a browser. We used Crossbench to run \speedometer
for 30 \emph{process executions}, where we close down the browser
and start a new browser. Crossbench records the wall-clock time of each individual
benchmark as well as the total time of the process execution \laurie{when does
the clock start and stop for the process execution?} \jake{i don't know.
Hannes, any idea?}.

We use Chrome version 112.0.55572 and V8 version 11.2.28. The benchmarks were
run on a Xeon E3-1240v6 3.7GHz CPU cores, with 32GB of RAM, and running Debian 8.
We disabled turbo boost and hyper-threading in the BIOS and set Linux's CPU
governor to \emph{performance} mode. We observed the \texttt{dmesg} after
benchmarking and did not observe any oddities such as the machine overheating.


\subsection{Results}

\cref{table:nocompact} shows the results from our first experiment,
which is a straightforward comparison of V8 before and after our migration. We achieve
a total speedup of \nocompacttotalpdiff\% \laurie{i believe there is no statistical difference between handles and direct refs in this table: their confidence intervals overlap AFAICS?} when using direct references over
handles. \jake{It's quite hard to see where the costs of this are in the perf
data. I think it's likely coming from stack scanning costs, but I can't be
certain. I might be doing something silly or looking at the diff incorrectly.}

\cref{table:min} shows the results from our second experiment, where we
try to take the costs of collection out of the comparison. Direct references
are \mintotalpdiff\% \laurie{it's better to put the `\%` in the macro
definition to avoid the space appearing here} faster than handles.
To make sense of these results, we first looked at
linux-perf data from a single run of the \jquery story, which was \minjquerypdiff\% faster
when using direct references over handles. The \jquery story stresses interactions between
the DOM heap on the renderer side (Blink) and V8's JavaScript heap. We saw
approximately a 15\% speedup on most functions across the the embedder API layer,
a part of V8 where we performed a near total migration (Section \ref{migrated}).

The \emberjsdebug story was the worst performing benchmark in
\cref{table:min}, with direct references on average \minemberjsdebugpdiff\%
slower. Though this is within the margin of error, the perf data shows clear
performance regressions on the functions which deal with the following parts of
V8: entry to parts of the JIT compiler; a section of the inline cache; and
JavaScript debug objects. None of these sections have been migrated away from
handles, however, they all interact heavily with parts of V8 that now use direct
references. In particular, the execution engine (responsible for making JS
function calls) and getters and setters on JavaScript objects themselves. It is
very likely that this slowdown is the result of bottlenecks in V8 where creating
\lstinline{Handle} instances from \lstinline{DirectRef}s are necessary.

\jake{I can provide more perf details. Or is that enough?}


\makeatletter
\newcommand*\ExpandableInput[1]{\@@input#1 }
\makeatother

 \begin{table*}[t]
     \begin{tabular}{lll}
 \toprule
         Benchmark & Handles (ms) & Direct Refs (ms) \\
 \midrule
\ExpandableInput{./table_min}
 \bottomrule
 \end{tabular}
\caption{Results from our second experiment: teasing out the mutator
   performance of handles and direct references without GC costs. We ran
   \speedometer 30 times and report 99\% confidence intervals. This clearly
   shows that, when GC is remove from the equation, direct references are
   faster.}
\label{table:min}
\end{table*}

\laurie{not sure i understand this paragraph}
89\% of garbage collections in Chrome are happening without a stack when we are on the message loop.
Therefore making stack scanning conservative may not be an issue in practise.
Add Speedometer numbers how often we finalize with a stack there.
Maybe add browsing benchmark numbers to confirm impact\hannes{discuss with the team}.


\section{Threats to Validity}

We have only migrated about half of V8 to direct references. It is possible
that there are challenges in the remainder of the code base that would change
our view of the merits of direct references. We have tried to mitigate this by
migrating what we consider to be the most challenging parts of V8 first.
While V8 is only partly migrated, there are also performance overheads
involved in moving between migrated and unmigrated code (e.g.~the null
checks explained in \cref{null_check}): these are very likely to lower our view
of direct references performance.

JavaScript VMs such as V8 are somewhat unusual amongst modern VMs in two
respects: JavaScript is single-threaded; and it is based around an event loop
which naturally gives frequent opportunities to perform garbage collection
without pointers existing on the C stack. These two factors may have a
notable impact on our understand of the performance merits of direct references
and handles. We would have to migrate at least one non-JavaScript VM
(e.g.~HotSpot) to understand whether these are significant factors or not.

There are two differences between the configurations of Chrome/V8 we run and
those run by normal users. First, we had to disable V8's young generation,
because it does not support pinning immovable objects which were located by
conservative stack scanning. Second, we
do not have the ability to generate or use the profile-guided-optimisation data
that production builds have access to.
At best, these two factors mean that our Chrome/V8 builds will be slower than
their production cousins; at worst, it could be that this changes the relative
performance of handles and direct references.

\speedometer is a small, synthetic benchmark suite: like any finite-sized
benchmark suite, it can not tell us about the performance of all possible
programs. However, its ubiquity does mean that its strengths and weaknesses as
a benchmark suite are widely acknowledged, and it allows a good degree of
comparison across JavaScript VMs and browsers.

Objects identified via conservative stack scanning cannot be moved. However, at
the moment we haven't implemented per-object pinning, so we are forced to
prevent heap compaction whenever GC occurs and there might be pointers on the
stack. This is not as onerous as it would be in most VMs, as most GC cycles
\jake{Hannes says 89\%, but I don't have a source for this, can we get one?}
occur during the JavaScript event loop, when there can be, by design,
no pointers on the call stack. \jake{Should we expand on this?}


\section{Related Work}
\label{sec:related}

The relative merits of direct pointers and handles have a long history in GC
and VMs, though quantitative comparisons are few in number. Generalising
somewhat, one technique or the other has tended to hold sway either at
different points in time, or within different communities.

Handles were commonly used in early Smalltalk VMs \laurie{let's cite at least
one Smalltalk VM using handles, ideally whichever is the earliest we know of}
\laurie{do we know why they were used in Smalltalk VMs? for moving i presume?}
and remain common in VMs that descend from that tradition (e.g.~Self,
HotSpot, Dart). \laurie{not sure if the rest of this is useful} HotSpot -- the now \emph{de facto}
standard JVM -- uses handles for C++ code \laurie{cite at least the glossary}.
Separately, HotSpot also uses a different kind of handles in the Java Native
Interface (JNI), its foreign function interface. Java objects are added to a
table of registered objects and accessed through a level of indirection when
used externally. The collector treats this table as an additional set of roots.
\laurie{i would not be surprised if the JNI handles are the same as the C++
handles}

\laurie{let's try relate this to our paper}
To the best of our knowledge, the most comprehensive study of handles
is~\cite{kalibera11handles}. They showed how moving handle-based VMs do not suffer
from \textit{copy reserve overhead}, where the old copy of a moved object must
be kept around until the mutator only holds references to the new object
location. Instead, with handles, the space used by evacuated objects can be
reused immediately as only the handle's address needs updating. With careful
optimisations, they showed that a handle-based system has same execution time
overheads as a Brooks-style compacting collector. Some of the ways they describe
that handles can be optimisated are: using \textit{fat handles} to store
commonly accessed header information along with the object pointer; grouping
handles of objects that live and die together to reduce fragmentation in handle
blocks; and short-circuiting handles for pinned objects.

Firefox's JavaScript engine SpiderMonkey uses handles in a similar fashion to
V8~\cite{spidermonkey}. \laurie{i'm not sure which bits of the rest of the
paragraph are different than V8.} Spidermonkey
developers must ensure that each pointer to a JavaScript object on the stack is
registered as root for garbage collection. Additional references to a JavaScript
object can then be created, which access it indirectly through the originally
rooted value. Rooted values are registered in lists based on scope, with
persistent handles available for longer living roots. Handles cannot be stored
on the heap as they could end up outliving their root. SpiderMonkey
uses a static analysis tool to make sure that raw pointers are not accidentally
put on the stack when a garbage collection can take place. The Gecko Rendering
engine is used in firefox. It is written in C++ and manages its DOM heap with
reference counting. It implements a cycle collector based on
\cite{bacon01concurrent}.

The use of direct pointers implicitly requires conservative scanning of at
least the stack. However, most (perhaps all) programming languages and most
(perhaps all) compilers have rules which suggest that conservative stack
scanning is undefined behaviour. Most obviously, there is no way of writing a
conservative stack scanner in C/C++ which is not undefined behaviour. In
practise, fortunately, conservative stack scanning `works', as perhaps best
evidenced by the long history of the Boehm-Demers-Weiser (BDW)
GC~\cite{boehm88garbage} which uses conservative scanning for the stack and
other possible location of GC roots (though note that the first conservative
scanning GC appears to have been~\cite{caplinger88memory}). BDW has been ported
to most platforms in its long history, and used by a wide variety of software
(including relatively modern software such as Inkscape): it is plausible that
its very existence has been the reason why conservative stack scanning
continues to be supported in practise across languages and compilers.

\jake{Should I mention oilpan here? Or is it too
"Chrome" to be considered related?} \laurie{i think it's fine to mention it, though 1 or 2 sentences is probably enough} Other conservative collectors for unmanaged
languages are the .

Safari's underlying engine WebKit (of which the JavaScript VM, JSC, is a part)
use direct pointers and conservative stack scanning~\cite{pizlo17riptide},
which is very similar to what we propose for V8 in this paper. WebKit has had
at least two different GCs over time (the current GC, \cite{pizlo17riptide},
mostly runs concurrently to JavaScript execution, similarly to V8), but this has not
changed the details relevant to this paper. Unlike V8, WebKit's GC never
moves objects, even across generations (instead using `sticky mark bits' that implicitly mark a given
object as belonging to a certain generation).
Where V8 only uses precise stack scanning
for C/C++ code, using stackmaps for JIT compiled code, WebKit uses conservative stack scanning for
both C/C++ code and JIT compiled code, marginally increasing the chances of
`accidental' object marking.

\laurie{is there anything novel about this paper w.r.t. handles? if it's just
that it uses handles to allow moving, that seems the same as all the earlier
handle-using systems?}
Compact-fit\cite{craciunas08compacting} is a compacting allocator which uses handles to allow objects to
move. It consists of two conceptual memory layers: the \textit{abstract address
space}; and the \textit{concrete address space}. Direct pointers to objects
allocated in the concrete address space are not possible. The application
operates on pointers to the abstract address space which are managed by handles.
Object headers have back-pointers to their canonical handle.

%TODO: Managed language collectors:
% - Fast CSS: https://users.cecs.anu.edu.au/~steveb/pubs/papers/consrc-oopsla-2014.pdf
% - Hybrid CSS: https://www.usenix.org/legacy/events/jvm01/full_papers/barabash/barabash.pdf
% - Forkscan Reclamation: https://dspace.mit.edu/bitstream/handle/1721.1/123336/forkscan-eurosys-17.pdf
% - Sound GC for C using taint analysis: https://dl.acm.org/doi/pdf/10.1145/3428244

\section{Conclusions}
\label{sec:conclusion}

% \textbf{Acknowledgements:} Hughes was funded by a research gift from Google. Tratt
% was funded by the Shopify / Royal Academy of Engineering Research Chair in Language
% Engineering.

\bibliographystyle{plain}
\bibliography{bib}

\end{document}
